{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SoftmaxRegression",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZsuhsxgWiWU"
      },
      "source": [
        "import numpy as np\n",
        "from math import log2\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PL1L8vigLLNW"
      },
      "source": [
        "#data generator for classification"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rhrv6JeqHUTS"
      },
      "source": [
        "#utils\n",
        "def split_train_valid_test(data,valid_ratio,test_ratio):\n",
        "  ```\n",
        "    TODO: \n",
        "      * Shuffle data\n",
        "      * Split \n",
        "              - Train\n",
        "              - Valid\n",
        "              - Test\n",
        "  ```\n",
        "  shuffled_indcies=np.random.permutation(len(data))\n",
        "  valid_set_size= int(len(data)*valid_ratio)\n",
        "  valid_indcies=shuffled_indcies[:valid_set_size]\n",
        "  test_set_size= int(len(data)*test_ratio)\n",
        "  test_indcies=shuffled_indcies[valid_set_size:test_set_size+valid_set_size]\n",
        "  train_indices=shuffled_indcies[test_set_size:]\n",
        "  return data.iloc[train_indices],data.iloc[valid_indcies],data.iloc[test_indcies]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3G9M6QNYr9K"
      },
      "source": [
        "class SoftmaxRegression():\n",
        "  def __init__(self):\n",
        "    self.W = None\n",
        "    self.best_W = None\n",
        "    self.init_w_mean = 0\n",
        "    self.init_w_std = 1\n",
        "    self.train_mode = False\n",
        "    self.train_history = []\n",
        "    self.valid_history = []\n",
        "\n",
        "  @staticmethod\n",
        "  def softmax(Z):\n",
        "    EZ = np.exp(Z)\n",
        "    SUM = np.sum(Z, axis=1, keepdims=True)\n",
        "    return EZ/SUM\n",
        "  \n",
        "  @staticmethod\n",
        "  def cross_entropy(Y_hat, y):\n",
        "    return -sum([Y_hat[i]*log2(y[i]) for i in range(len(Y_hat))])\n",
        "\n",
        "  @staticmethod\n",
        "  def one_hot_encoder(K, batch_label):\n",
        "    pass\n",
        "  \n",
        "  def __init_params(self, number_of_classes, number_of_features):\n",
        "    self.W = np.random.normal(self.init_w_mean, self.init_w_std, (number_of_classes, number_of_features))\n",
        "    # add bias = 0\n",
        "    self.W[:, 0] = 0\n",
        "\n",
        "  def predict(self, X):\n",
        "    if self.train_mode:\n",
        "      W = self.W\n",
        "    else:\n",
        "      W = self.best_W\n",
        "    \n",
        "    Z = np.dot(X, W.T)\n",
        "    Y_hat = self.softmax(Z)\n",
        "    return Y_hat\n",
        "\n",
        "  def validate(self, X, y):\n",
        "    Z = np.dot(X, self.W.T)\n",
        "    Y_hat = self.softmax(Z)\n",
        "    loss = self.cross_entropy(Y_hat)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def fit(self,\n",
        "          X_train, y_train,\n",
        "          X_valid, y_valid,\n",
        "          eta=0.001,\n",
        "          BATCH_SIZE=1,\n",
        "          NUM_EPOCHES=100,\n",
        "          init_w_mean=0,\n",
        "          init_w_std=1,\n",
        "          weight_decay=0.0001,\n",
        "          weight_decay_type='l2',\n",
        "          nfreq=500):\n",
        "    #Initialize\n",
        "    N, M = X_train.shape\n",
        "    K = int(np.max(y_train)) + 1\n",
        "    self.init_w_mean = init_w_mean\n",
        "    self.init_w_std = init_w_std\n",
        "    self.__init_params(K, M)\n",
        "\n",
        "    #Assert for BATCH_SOZE\n",
        "    if BATCH_SIZE < 1:\n",
        "      BATCH_SIZE = 1\n",
        "      print('BATCH_SIZE < 1 => BATCH_SIZE = 1')\n",
        "    elif BATCH_SIZE > N:\n",
        "      BATCH_SIZE = N\n",
        "      print('BATCH_SIZE > N => BATCH_SIZE = N')\n",
        "    \n",
        "    #Initialize best_loss\n",
        "    best_loss = float(\"inf\")\n",
        "    num_batches = N // BATCH_SIZE #last batch contains all remaining sample\n",
        "    batch_idx = 0\n",
        "    train_loss = 0\n",
        "    self.train_history = []\n",
        "    self.valid_history = []\n",
        "    nepoch = 0\n",
        "    while True:\n",
        "      if batch_idx > num_batches - 1:\n",
        "        nepoch += 1\n",
        "        valid_loss = self.validate(X_valid, y_valid)\n",
        "        if nepoch%nfreq == 0:\n",
        "          s_epoch = '{:>7d}'.format(nepoch)\n",
        "          s_train_loss = '{:>8.5f}'.format(train_loss)\n",
        "          s_valid_loss = '{:>8.5f}'.format(valid_loss)\n",
        "          log = 'Epoch: {:s} # Train loss: {:s} # Valid loss: {:s}'.format(s_epoch, s_train_loss, s_valid_loss)\n",
        "          print(log)\n",
        "\n",
        "        self.train_history.append(train_loss)\n",
        "        train_loss = 0\n",
        "        self.valid_history.append(valid_loss)\n",
        "\n",
        "        if valid_loss < best_loss:\n",
        "          best_loss = valid_loss\n",
        "          self.best_W = self.W.copy()\n",
        "        \n",
        "        if nepoch > NUM_EPOCHES:\n",
        "          break;\n",
        "\n",
        "        #Decay\n",
        "        #eta = eta*0.9**(nepoch/20)\n",
        "\n",
        "        #Shuffle data\n",
        "        shuffled_idx = np.random.permutation(N)\n",
        "        X_train = X_train[shuffled_idx, :1]\n",
        "        y_train = y_train[shuffled_idx]\n",
        "\n",
        "        #reset\n",
        "        batch_idx = 0\n",
        "        start_idx = batch_idx*BATCH_SIZE\n",
        "        end_idx = min(N, start_idx + BATCH_SIZE)\n",
        "\n",
        "      elif batch_idx == num_batches - 1:\n",
        "        start_idx = batch_idx * BATCH_SIZE\n",
        "        end_idx = N\n",
        "\n",
        "      else:\n",
        "        start_idx = batch_index * BATCH_SIZE\n",
        "        end_indx = min(N, start_idx + BATCH_SIZE)\n",
        "      \n",
        "      #Batch data + labels\n",
        "      batch_data = X_train[start_idx:end_idx, :]\n",
        "      batch_label = y_train[start_idx:end_idx, :]\n",
        "\n",
        "      #Forward\n",
        "      batch_pred = self.predict(batch_data)\n",
        "\n",
        "      #Loss\n",
        "      batch_loss = cross_entropy(batch_pred)\n",
        "\n",
        "      #Regularize\n",
        "      WW = self.W[:, 1:] #Do not use bias for Regularization\n",
        "      if weight_decay_type == 'l2':\n",
        "          l2 = np.sum(WW*WW)\n",
        "          batch_loss += weight_decay * l2\n",
        "      else:\n",
        "        l1 = np.sum(np.abs(WW))\n",
        "        batch_loss += weight_decay * l1\n",
        "      \n",
        "      train_loss += batch_loss\n",
        "\n",
        "      #Backward pass\n",
        "      if weight_decay_type == 'l2':\n",
        "        regu = self.W.copy()\n",
        "        regu[:, 0] = 0\n",
        "      else:\n",
        "        regu = self.W.copy()\n",
        "        regu[:, 0] = 0\n",
        "        regu[regu>0] = 1\n",
        "        regu[regu<0] = -1\n",
        "        regu[regu==0] = 0\n",
        "      \n",
        "      DZ = batch_pred - one_hot_encoder(K, batch_label)\n",
        "      DW = np.dot(DZ.T, batch_data)\n",
        "      DW = (1.0/batch_data.shape[0]) * DW + weight_decay * regu\n",
        "\n",
        "      self.W = self.W - eta * DW\n",
        "      \n",
        "      #next batch\n",
        "      batch_idx += 1\n",
        "    \n",
        "    self.train_mode = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDtzhk_8Q_54"
      },
      "source": [
        "class LabelTransformer(object):\n",
        "\n",
        "    def __init__(self, n_classes:int=None):\n",
        "        self.n_classes = n_classes\n",
        "\n",
        "    @property\n",
        "    def n_classes(self):\n",
        "        return self.__n_classes\n",
        "\n",
        "    @n_classes.setter\n",
        "    def n_classes(self, K):\n",
        "        self.__n_classes = K\n",
        "        self.__encoder = None if K is None else np.eye(K)\n",
        "\n",
        "    @property\n",
        "    def encoder(self):\n",
        "        return self.__encoder\n",
        "\n",
        "    def encode(self, class_indices:np.ndarray):\n",
        "        \"\"\"\n",
        "        encode class index into one-of-k code\n",
        "        Parameters\n",
        "        ----------\n",
        "        class_indices : (N,) np.ndarray\n",
        "            non-negative class index\n",
        "            elements must be integer in [0, n_classes)\n",
        "        Returns\n",
        "        -------\n",
        "        (N, K) np.ndarray\n",
        "            one-of-k encoding of input\n",
        "        \"\"\"\n",
        "        if self.n_classes is None:\n",
        "            self.n_classes = np.max(class_indices) + 1\n",
        "\n",
        "        return self.encoder[class_indices]\n",
        "\n",
        "    def decode(self, onehot:np.ndarray):\n",
        "        return np.argmax(onehot, axis=1)\n",
        "\n",
        "class SoftmaxRegression(object):\n",
        "    \"\"\"\n",
        "    Softmax regression model\n",
        "    aka\n",
        "    multinomial logistic regression,\n",
        "    multiclass logistic regression,\n",
        "    maximum entropy classifier.\n",
        "    y = softmax(X @ W)\n",
        "    t ~ Categorical(t|y)\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _softmax(a):\n",
        "        a_max = np.max(a, axis=-1, keepdims=True)\n",
        "        exp_a = np.exp(a - a_max)\n",
        "        return exp_a / np.sum(exp_a, axis=-1, keepdims=True)\n",
        "\n",
        "    def fit(self, X:np.ndarray, t:np.ndarray, max_iter:int=100, learning_rate:float=0.1):\n",
        "        \"\"\"\n",
        "        maximum likelihood estimation of the parameter\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (N, D) np.ndarray\n",
        "            training independent variable\n",
        "        t : (N,) or (N, K) np.ndarray\n",
        "            training dependent variable\n",
        "            in class index or one-of-k encoding\n",
        "        max_iter : int, optional\n",
        "            maximum number of iteration (the default is 100)\n",
        "        learning_rate : float, optional\n",
        "            learning rate of gradient descent (the default is 0.1)\n",
        "        \"\"\"\n",
        "        if t.ndim == 1:\n",
        "            t = LabelTransformer().encode(t)\n",
        "        self.n_classes = np.size(t, 1)\n",
        "        W = np.zeros((np.size(X, 1), self.n_classes))\n",
        "        for _ in range(max_iter):\n",
        "            W_prev = np.copy(W)\n",
        "            y = self._softmax(X @ W)\n",
        "            grad = X.T @ (y - t)\n",
        "            W -= learning_rate * grad\n",
        "            if np.allclose(W, W_prev):\n",
        "                break\n",
        "        self.W = W\n",
        "\n",
        "    def proba(self, X:np.ndarray):\n",
        "        \"\"\"\n",
        "        compute probability of input belonging each class\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (N, D) np.ndarray\n",
        "            independent variable\n",
        "        Returns\n",
        "        -------\n",
        "        (N, K) np.ndarray\n",
        "            probability of each class\n",
        "        \"\"\"\n",
        "        return self._softmax(X @ self.W)\n",
        "\n",
        "    def classify(self, X:np.ndarray):\n",
        "        \"\"\"\n",
        "        classify input data\n",
        "        Parameters\n",
        "        ----------\n",
        "        X : (N, D) np.ndarray\n",
        "            independent variable to be classified\n",
        "        Returns\n",
        "        -------\n",
        "        (N,) np.ndarray\n",
        "            class index for each input\n",
        "        \"\"\"\n",
        "        return np.argmax(self.proba(X), axis=-1)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}